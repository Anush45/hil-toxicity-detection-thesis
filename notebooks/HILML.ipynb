{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb387d83-ba7c-4b15-ad30-12fda978e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "civil_df = pd.read_csv('civil_comments.csv')\n",
    "toxic_df = pd.read_csv('toxic_comments.csv')\n",
    "print(\"🟦 Civil Comments Dataset (Unintended Bias):\")\n",
    "display(civil_df.head(3))\n",
    "print(f\"Shape: {civil_df.shape}\")\n",
    "print(f\"Columns: {civil_df.columns.tolist()}\")\n",
    "print(\"\\n🟥 Toxic Comments Dataset (Toxic Challenge):\")\n",
    "display(toxic_df.head(3))\n",
    "print(f\"Shape: {toxic_df.shape}\")\n",
    "print(f\"Columns: {toxic_df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6afee4-c683-4090-bb25-808c4b3ae2e2",
   "metadata": {},
   "source": [
    "# Importing and Loadking Data\n",
    "1 --> Civil comments -> Source - Jigsaw, Purpose -> Bias and Fairness analysis. Text comments from real platform.\n",
    "2 --> Toxic comments -> Source - kaggle, Purpose - > Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f759c1b-36f2-407d-933a-41c2c1644b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "toxic_df = pd.read_csv(\"toxic_comments.csv\")\n",
    "print(\"Dataset shape:\", toxic_df.shape)\n",
    "print(\"\\n Column names:\", toxic_df.columns.tolist())\n",
    "toxic_df.head()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(toxic_df.isnull().sum())\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "print(\"\\n Label distribution:\")\n",
    "print(toxic_df[label_cols].sum().sort_values(ascending=False))\n",
    "toxic_df['num_labels'] = toxic_df[label_cols].sum(axis=1)\n",
    "print(\"\\n Comments with multiple labels:\")\n",
    "print(toxic_df['num_labels'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1c726-0301-450c-a096-59d1429d3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "civil_df = pd.read_csv(\"civil_comments.csv\")\n",
    "print(\"Dataset shape:\", civil_df.shape)\n",
    "print(\"\\nColumn names:\", civil_df.columns.tolist())\n",
    "print(\"\\n❓ Missing values per column:\")\n",
    "print(civil_df.isnull().sum().sort_values(ascending=False))\n",
    "civil_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a06bda-a8d4-4217-980c-ce2d744048fd",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3a8eb-09c9-424e-b4c3-3ec520360a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #Converting text into numbers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "df = pd.read_csv(\"toxic_comments.csv\")\n",
    "X = df['comment_text']\n",
    "y = df['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_prob = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a2774-272d-4a05-ab8e-c7933a680ba7",
   "metadata": {},
   "source": [
    "# A short Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f4273-9e4d-43f9-af5a-708f34e56bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = [\n",
    "#     \"You are so dumb and annoying\",               # likely toxic\n",
    "#     \"Thank you for your help, I appreciate it\",   # non-toxic\n",
    "#     \"What a stupid idea, no one cares\"            # likely toxic\n",
    "# ]\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pandas as pd\n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# X = vectorizer.fit_transform(comments)\n",
    "# tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# print(tfidf_df.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c124a-21a4-4024-9014-1982ec3e1d38",
   "metadata": {},
   "source": [
    "# Simulating Low-Confidence Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d888f-716a-4ec4-b113-b4ff8c417d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lower_thresh = 0.4\n",
    "upper_thresh = 0.6\n",
    "low_conf_mask = (y_prob >= lower_thresh) & (y_prob <= upper_thresh)\n",
    "X_uncertain = X_test[low_conf_mask]\n",
    "y_uncertain_true = y_test[low_conf_mask]\n",
    "y_uncertain_prob = y_prob[low_conf_mask]\n",
    "print(f\"Total low-confidence predictions: {len(X_uncertain)}\")\n",
    "print(f\"Percentage of test set: {len(X_uncertain) / len(X_test):.2%}\") #Result is out of 20% trained Jigsaw train.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25b6b7-61cb-4988-892e-a23dabe4110f",
   "metadata": {},
   "source": [
    "# How uncertain is the Model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedd178-32b5-40a9-a5fd-1e100864e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "y_prob_simulated = np.concatenate([\n",
    "    np.random.beta(2, 8, size=5000),  \n",
    "    np.random.beta(8, 2, size=4000),  \n",
    "    np.random.normal(0.5, 0.05, size=500)  ])\n",
    "y_prob_simulated = np.clip(y_prob_simulated, 0, 1)  \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_prob_simulated, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.axvline(0.4, color='red', linestyle='--', label='Lower Threshold (0.4)')\n",
    "plt.axvline(0.6, color='red', linestyle='--', label='Upper Threshold (0.6)')\n",
    "plt.title('Model Prediction Confidence Distribution')\n",
    "plt.xlabel('Predicted Probability of Toxicity')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615dc26f-5a37-42cc-9793-1abfd674c5f7",
   "metadata": {},
   "source": [
    "# Simulate Human Feedback & Retrain the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885181f2-07fd-47d4-a487-7e51229df503",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uncertain_cleaned = X_uncertain.astype(str)\n",
    "X_uncertain_tfidf = vectorizer.transform(X_uncertain_cleaned)\n",
    "print(\"Uncertain shape:\", X_uncertain_tfidf.shape)\n",
    "print(\"Vectorizer type:\", type(vectorizer))\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
    "from scipy.sparse import vstack\n",
    "X_augmented = vstack([X_train_tfidf, X_uncertain_tfidf])\n",
    "y_augmented = pd.concat([y_train.reset_index(drop=True), y_uncertain_true.reset_index(drop=True)])\n",
    "retrained_model = LogisticRegression()\n",
    "retrained_model.fit(X_augmented, y_augmented)\n",
    "y_pred_new = retrained_model.predict(X_test_tfidf)\n",
    "y_prob_new = retrained_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "print(\"\\n--- After Human Feedback Retraining ---\")\n",
    "print(classification_report(y_test, y_pred_new))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_prob_new))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0677c-5811-40e1-9ff9-a19cfdf48fb4",
   "metadata": {},
   "source": [
    "# Performace Comparison after Human(y_tets) Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f203cd-1fb9-4b50-898a-5f32317e9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "metrics = ['Precision', 'Recall', 'F1-score', 'AUC']\n",
    "before = [0.90, 0.61, 0.73, 0.9660]\n",
    "after = [0.92, 0.64, 0.76, 0.9668]\n",
    "x = np.arange(len(metrics)) \n",
    "width = 0.35  \n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars1 = ax.bar(x - width/2, before, width, label='Before Feedback', color='lightcoral')\n",
    "bars2 = ax.bar(x + width/2, after, width, label='After Feedback', color='mediumseagreen')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance: Before vs After Human Feedback')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.2f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),  # Offset label above bar\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976158d-5658-4f5f-a1fb-8b1b907160b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"civil_comments.csv\")\n",
    "civil_df = df.sample(n=10000, random_state=42).copy()\n",
    "\n",
    "civil_df.to_csv(\"civil_comments_sample.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d692e33-d134-49b2-bbef-5e7353d32323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "civil_df = pd.read_csv(\"civil_comments.csv\")\n",
    "civil_df['comment_text'] = civil_df['comment_text'].astype(str).fillna(\"\")\n",
    "\n",
    "X_civil = vectorizer.transform(civil_df['comment_text'])\n",
    "\n",
    "civil_df['model_prob'] = model.predict_proba(X_civil)[:, 1]\n",
    "civil_df['model_pred'] = model.predict(X_civil)\n",
    "\n",
    "low_thresh = 0.4\n",
    "high_thresh = 0.6\n",
    "uncertain_mask = (civil_df['model_prob'] >= low_thresh) & (civil_df['model_prob'] <= high_thresh)\n",
    "civil_uncertain = civil_df[uncertain_mask].copy()\n",
    "\n",
    "civil_uncertain[['comment_text', 'model_prob']].to_csv(\"uncertain_civil_comments.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Extracted {len(civil_uncertain)} uncertain comments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1883911-a4e8-4c62-95ac-98366da8c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "civil_uncertain = pd.read_csv(\"uncertain_civil_comments.csv\")\n",
    "civil_uncertain['word_count'] = civil_uncertain['comment_text'].str.split().apply(len)\n",
    "civil_filtered = civil_uncertain[(civil_uncertain['word_count'] >= 10) & (civil_uncertain['word_count'] <= 30)]\n",
    "\n",
    "# 🚀 Add model prediction column (0 or 1)\n",
    "civil_filtered['model_label'] = (civil_filtered['model_prob'] >= 0.5).astype(int)  # or use your actual predictions if available\n",
    "\n",
    "# Sample 30 comments\n",
    "civil_form_sample = civil_filtered[['comment_text', 'model_prob', 'model_label']].sample(n=50, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display for form use\n",
    "for i, row in civil_form_sample.iterrows():\n",
    "    label = \"Toxic\" if row['model_label'] == 1 else \"Non-Toxic\"\n",
    "    print(f\"\\nComment {i+1}:\")\n",
    "    print(f\"Text: {row['comment_text']}\")\n",
    "    print(f\"Model Prediction: {label}\")\n",
    "    print(f\"Model Confidence: {row['model_prob']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd7048-2683-4177-bc04-fded892c7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identity Subgroups\n",
    "civil_df = pd.read_csv(\"civil_comments.csv\")\n",
    "civil_df['comment_text'] = civil_df['comment_text'].astype(str)\n",
    "civil_df['word_count'] = civil_df['comment_text'].str.split().apply(len)\n",
    "X_civil = vectorizer.transform(civil_df['comment_text'])\n",
    "civil_df['model_prob'] = model.predict_proba(X_civil)[:, 1]\n",
    "civil_df['model_pred'] = model.predict(X_civil)\n",
    "identity_columns = [\n",
    "    'male', 'female', 'transgender', 'heterosexual',\n",
    "    'homosexual_gay_or_lesbian', 'bisexual', 'christian', 'jewish', 'muslim',\n",
    "    'black', 'white', 'asian', 'latino', 'psychiatric_or_mental_illness', 'hindu'\n",
    "]\n",
    "group_counts = civil_df[identity_columns].sum().sort_values(ascending=False)\n",
    "print(\"\\n Subgroup Mentions:\\n\")\n",
    "print(group_counts)\n",
    "print(\"\\n Avg predicted toxicity probability by subgroup:\\n\")\n",
    "for col in identity_columns:\n",
    "    avg_prob = civil_df[civil_df[col] == 1]['model_prob'].mean()\n",
    "    print(f\"{col:<30}: {avg_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cabbff-15a7-47d4-9378-8dd464433a2a",
   "metadata": {},
   "source": [
    "## Basic Analysis of Human Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925af50a-9453-438c-84ee-2e6cc26a1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the file after kernel reset\n",
    "file_path = \"human_feedback_responses.csv\"\n",
    "df_feedback = pd.read_csv(file_path)\n",
    "\n",
    "# Show basic info and first few rows\n",
    "df_feedback.info(), df_feedback.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d818b3a-d033-4b50-a7f7-cc2fd338a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the final duplicate column (which contains only NaNs)\n",
    "df_feedback_cleaned = df_feedback.iloc[:, :-1]\n",
    "\n",
    "# Remove timestamp column for analysis\n",
    "df_feedback_cleaned = df_feedback_cleaned.drop(columns=[\"Timestamp\"])\n",
    "\n",
    "# Standardize responses: Agree -> 1, Disagree -> 0, Not Sure -> NaN\n",
    "response_map = {\n",
    "    \"Agree\": 1,\n",
    "    \"Disagree\": 0,\n",
    "    \"Not Sure\": None\n",
    "}\n",
    "df_feedback_cleaned_numeric = df_feedback_cleaned.applymap(lambda x: response_map.get(x.strip()) if isinstance(x, str) else x)\n",
    "\n",
    "# Preview cleaned data\n",
    "df_feedback_cleaned_numeric.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf1bdc-d786-4b6c-abb1-4eadc3135be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall agreement rate (ignoring 'Not Sure' responses)\n",
    "total_valid_responses = df_feedback_cleaned_numeric.count().sum()\n",
    "total_agree = (df_feedback_cleaned_numeric == 1).sum().sum()\n",
    "overall_agreement_rate = total_agree / total_valid_responses\n",
    "\n",
    "# Also compute per-question agreement rate\n",
    "per_question_agreement = df_feedback_cleaned_numeric.apply(lambda col: (col == 1).sum() / col.count())\n",
    "\n",
    "overall_agreement_rate, per_question_agreement.sort_values(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60ef4c-0596-4179-8208-d8ddca606e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the uncertain_civil_comments.csv file\n",
    "# uncertain_path = \"/mnt/data/uncertain_civil_comments.csv\"\n",
    "# civil_uncertain = pd.read_csv(uncertain_path)\n",
    "\n",
    "# # Step 2: Filter comments between 10–30 words\n",
    "# civil_uncertain['word_count'] = civil_uncertain['comment_text'].str.split().apply(len)\n",
    "# civil_filtered = civil_uncertain[(civil_uncertain['word_count'] >= 10) & (civil_uncertain['word_count'] <= 30)]\n",
    "\n",
    "# # Step 3: Add model label based on 0.5 threshold\n",
    "# civil_filtered['model_label'] = (civil_filtered['model_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# # Step 4: Sample the same 30 comments (random_state=42)\n",
    "# civil_form_sample = civil_filtered[['comment_text', 'model_prob', 'model_label']].sample(n=30, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Display to confirm\n",
    "# civil_form_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dde4ff-d9b2-4e42-bc73-19b5b9df597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset human feedback if not already loaded\n",
    "feedback_path = \"human_feedback_responses.csv\"\n",
    "df_feedback = pd.read_csv(feedback_path)\n",
    "\n",
    "# Drop extra columns and clean response data again\n",
    "df_feedback_cleaned = df_feedback.drop(columns=[\"Timestamp\"])\n",
    "response_map = {\"Agree\": 1, \"Disagree\": 0, \"Not Sure\": None}\n",
    "df_feedback_numeric = df_feedback_cleaned.applymap(lambda x: response_map.get(x.strip()) if isinstance(x, str) else x)\n",
    "\n",
    "# Transpose to match comment-wise\n",
    "df_feedback_transposed = df_feedback_numeric.T\n",
    "df_feedback_transposed.columns = [f\"R{i+1}\" for i in range(df_feedback_transposed.shape[1])]\n",
    "df_feedback_transposed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add average human agreement score\n",
    "df_feedback_transposed[\"human_agreement_score\"] = df_feedback_transposed.mean(axis=1)\n",
    "\n",
    "# Derive human label: if > 0.5, majority agreed with model, else disagreed\n",
    "df_feedback_transposed[\"human_label\"] = (df_feedback_transposed[\"human_agreement_score\"] >= 0.5).astype(int)\n",
    "\n",
    "# Merge with model predictions\n",
    "corrected_df = civil_form_sample.copy()\n",
    "corrected_df[\"human_label\"] = df_feedback_transposed[\"human_label\"]\n",
    "corrected_df[\"agreement_score\"] = df_feedback_transposed[\"human_agreement_score\"]\n",
    "\n",
    "# Mark where model prediction differs from human label\n",
    "corrected_df[\"label_changed\"] = corrected_df[\"model_label\"] != corrected_df[\"human_label\"]\n",
    "\n",
    "print(corrected_df.head(10))  # Or use corrected_df to access the full DataFrame\n",
    "\n",
    "# Save corrected DataFrame to CSV\n",
    "# corrected_df.to_csv(\"corrected_human_feedback.csv\", index=False)\n",
    "\n",
    "# print(\"✅ File saved as 'corrected_human_feedback.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e26d41-9806-4980-a0c0-5152c24ab271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the uncertain civil comments (previously uploaded)\n",
    "uncertain_df = pd.read_csv(\"uncertain_civil_comments.csv\")\n",
    "uncertain_df['comment_text'] = uncertain_df['comment_text'].astype(str)\n",
    "\n",
    "# Filter non-toxic rows (model_label = 0) that are not in corrected_df\n",
    "non_toxic_candidates = uncertain_df.copy()\n",
    "non_toxic_candidates['model_label'] = (non_toxic_candidates['model_prob'] >= 0.5).astype(int)\n",
    "non_toxic_candidates = non_toxic_candidates[non_toxic_candidates['model_label'] == 0]\n",
    "\n",
    "# Remove rows that already exist in corrected_df\n",
    "existing_texts = corrected_df['comment_text'].tolist()\n",
    "non_toxic_candidates = non_toxic_candidates[~non_toxic_candidates['comment_text'].isin(existing_texts)]\n",
    "\n",
    "# Filter by readable length\n",
    "non_toxic_candidates['word_count'] = non_toxic_candidates['comment_text'].str.split().apply(len)\n",
    "non_toxic_filtered = non_toxic_candidates[(non_toxic_candidates['word_count'] >= 5) & (non_toxic_candidates['word_count'] <= 35)]\n",
    "\n",
    "# Sample 10 and assign label 0\n",
    "non_toxic_sample = non_toxic_filtered[['comment_text', 'model_prob', 'model_label']].sample(n=10, random_state=42)\n",
    "non_toxic_sample['human_label'] = None\n",
    "non_toxic_sample['final_label'] = 0  # Injected manually\n",
    "\n",
    "# Add to corrected_df\n",
    "corrected_augmented = pd.concat([corrected_df, non_toxic_sample], ignore_index=True)\n",
    "\n",
    "# Check label distribution\n",
    "corrected_augmented['final_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f7bae-472c-4e6a-be35-1ef2705f803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the feedback CSV\n",
    "df = pd.read_csv(\"human_feedback_responses.csv\")\n",
    "\n",
    "# # Count agreement responses (assuming column is named 'Agree?')\n",
    "# agree_counts = df['Agree?'].value_counts()\n",
    "\n",
    "# Plot\n",
    "colors = ['lightgreen', 'salmon']\n",
    "labels = ['Agreed with Model', 'Disagreed with Model']\n",
    "agree_counts = agree_counts.reindex(['Yes', 'No'])  # Ensure order\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(agree_counts, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "plt.title(\"Participant Agreement with Model Predictions\")\n",
    "plt.axis('equal')  # Equal aspect ratio ensures pie is circular\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd011d-aedb-4050-bb8a-5fc00a7ec239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Performance metrics\n",
    "metrics = ['Precision', 'Recall', 'F1-score', 'AUC']\n",
    "before = [0.90, 0.61, 0.73, 0.9660]\n",
    "after = [0.75, 1.00, 0.86, 0.25]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars1 = ax.bar(x - width/2, before, width, label='Before Feedback', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, after, width, label='After Feedback', color='darkorange')\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance: Before vs After Human Feedback')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add data labels\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.2f}', \n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), \n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4744c5-c648-49c6-ac64-5be661bb0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "LOW_THRESH   = 0.40    # uncertainty band lower\n",
    "HIGH_THRESH  = 0.60    # uncertainty band upper\n",
    "MAX_FEATS    = 10_000  # TF–IDF vocab size\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# 1) Load & Prepare Baseline Data (Jigsaw Toxic)\n",
    "# =========================\n",
    "toxic_df = pd.read_csv(\"toxic_comments.csv\")\n",
    "# Expecting columns: 'comment_text' + multilabels inc. 'toxic'\n",
    "assert 'comment_text' in toxic_df.columns, \"Missing 'comment_text' in toxic_comments.csv\"\n",
    "assert 'toxic' in toxic_df.columns, \"Missing 'toxic' column in toxic_comments.csv\"\n",
    "toxic_df['comment_text'] = toxic_df['comment_text'].astype(str).fillna(\"\")\n",
    "\n",
    "X = toxic_df['comment_text']\n",
    "y = toxic_df['toxic'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# TF–IDF (word + char) — same vectorizer reused everywhere\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=MAX_FEATS,\n",
    "    ngram_range=(1,2)  # unigrams + bigrams; char-ngrams optional if you want\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "\n",
    "# Baseline model\n",
    "baseline = LogisticRegression(max_iter=1000, n_jobs=None)\n",
    "baseline.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_base = baseline.predict(X_test_tfidf)\n",
    "y_prob_base = baseline.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print(\"=== Baseline (no HIL) ===\")\n",
    "print(classification_report(y_test, y_pred_base, digits=3))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_prob_base))\n",
    "\n",
    "# -------------------------\n",
    "# Baseline plots (saved to PNGs)\n",
    "# -------------------------\n",
    "def plot_baseline_curves(y_true, y_prob, y_pred, prefix=\"fig_4_1\"):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'--', lw=1)\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title('Baseline ROC Curve (LogReg + TF–IDF)')\n",
    "    plt.legend(loc='lower right'); plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout(); plt.savefig(f\"{prefix}a_baseline_roc.png\", dpi=300); plt.show()\n",
    "\n",
    "    # PR\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(rec, prec, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title('Baseline Precision–Recall Curve')\n",
    "    plt.legend(loc='lower left'); plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout(); plt.savefig(f\"{prefix}b_baseline_pr.png\", dpi=300); plt.show()\n",
    "\n",
    "    # Confusion\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(cm, cmap='Greys')\n",
    "    plt.title('Baseline Confusion Matrix (thr=0.5)')\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_labels = ['Non-Toxic (0)', 'Toxic (1)']\n",
    "    plt.xticks([0,1], tick_labels, rotation=20); plt.yticks([0,1], tick_labels)\n",
    "    for (i,j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha='center', va='center', fontsize=12)\n",
    "    plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "    plt.tight_layout(); plt.savefig(f\"{prefix}c_baseline_confusion.png\", dpi=300); plt.show()\n",
    "\n",
    "    # Metric bars\n",
    "    P = precision_score(y_true, y_pred)\n",
    "    R = recall_score(y_true, y_pred)\n",
    "    F = f1_score(y_true, y_pred)\n",
    "    A = roc_auc_score(y_true, y_prob)\n",
    "    metrics = ['Precision','Recall','F1','AUC']; vals=[P,R,F,A]\n",
    "    plt.figure(figsize=(7,5))\n",
    "    bars = plt.bar(range(len(metrics)), vals)\n",
    "    for b,v in zip(bars, vals):\n",
    "        plt.text(b.get_x()+b.get_width()/2, v+0.02, f\"{v:.2f}\", ha='center')\n",
    "    plt.xticks(range(len(metrics)), metrics); plt.ylim(0,1.05)\n",
    "    plt.ylabel('Score'); plt.title('Baseline Performance')\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout(); plt.savefig(f\"{prefix}d_baseline_metrics.png\", dpi=300); plt.show()\n",
    "\n",
    "plot_baseline_curves(y_test, y_prob_base, y_pred_base)\n",
    "\n",
    "# =========================\n",
    "# 2) Score Civil Comments; Extract Uncertain Band\n",
    "# =========================\n",
    "civil_df = pd.read_csv(\"civil_comments.csv\")\n",
    "assert 'comment_text' in civil_df.columns, \"Missing 'comment_text' in civil_comments.csv\"\n",
    "civil_df['comment_text'] = civil_df['comment_text'].astype(str).fillna(\"\")\n",
    "\n",
    "X_civil_tfidf = vectorizer.transform(civil_df['comment_text'])\n",
    "civil_df['model_prob'] = baseline.predict_proba(X_civil_tfidf)[:, 1]\n",
    "civil_df['model_label'] = (civil_df['model_prob'] >= 0.50).astype(int)\n",
    "\n",
    "uncertain_mask = (civil_df['model_prob'] >= LOW_THRESH) & (civil_df['model_prob'] <= HIGH_THRESH)\n",
    "civil_uncertain = civil_df.loc[uncertain_mask, ['comment_text','model_prob','model_label']].copy()\n",
    "civil_uncertain.to_csv(\"uncertain_civil_comments.csv\", index=False)\n",
    "print(f\"Extracted uncertain comments: {len(civil_uncertain)} \"\n",
    "      f\"({100*len(civil_uncertain)/len(civil_df):.2f}% of Civil).\")\n",
    "\n",
    "# Optional: prepare a small, readable sample for the Google Form\n",
    "def prepare_form_sample(df_uncertain, n=30, min_words=10, max_words=30, seed=RANDOM_STATE, path=\"civil_form_sample.csv\"):\n",
    "    tmp = df_uncertain.copy()\n",
    "    tmp['word_count'] = tmp['comment_text'].str.split().apply(len)\n",
    "    tmp = tmp[(tmp['word_count'] >= min_words) & (tmp['word_count'] <= max_words)]\n",
    "    if len(tmp) < n:\n",
    "        n = len(tmp)\n",
    "    sample = tmp.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "    sample.to_csv(path, index=False)\n",
    "    return sample\n",
    "\n",
    "# Uncomment if you want to (re)create the form sample:\n",
    "# form_sample = prepare_form_sample(civil_uncertain, n=30)\n",
    "\n",
    "# =========================\n",
    "# 3) Parse Human Feedback (Google Form -> CSV)\n",
    "# =========================\n",
    "# expected file from Google Sheets export:\n",
    "HF_PATH = \"human_feedback_responses.csv\"\n",
    "if os.path.exists(HF_PATH):\n",
    "    hf_raw = pd.read_csv(HF_PATH)\n",
    "else:\n",
    "    hf_raw = pd.DataFrame()  # keep empty if not provided yet\n",
    "\n",
    "def long_from_form(df):\n",
    "    \"\"\"\n",
    "    Flexible parser:\n",
    "    - Keeps 'Timestamp' if present (ignored in logic).\n",
    "    - Treats every non-Timestamp column as a question.\n",
    "    - Each column header should contain the full comment text (as you set in the form).\n",
    "    - Cell values are one of: 'Agree', 'Disagree', 'Not Sure' (case-insensitive).\n",
    "    Returns: long df with columns [comment_text, response]\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['comment_text','response'])\n",
    "\n",
    "    cols = [c for c in df.columns if str(c).lower() != 'timestamp']\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        for c in cols:\n",
    "            resp = str(row[c]).strip() if pd.notnull(row[c]) else \"\"\n",
    "            if resp == \"\":\n",
    "                continue\n",
    "            records.append({\n",
    "                'comment_text': str(c).strip(),  # header contains the item\n",
    "                'response'    : resp\n",
    "            })\n",
    "    long_df = pd.DataFrame.from_records(records)\n",
    "    return long_df\n",
    "\n",
    "hf_long = long_from_form(hf_raw)\n",
    "\n",
    "def map_response_to_label(resp: str, model_label_for_item: int) -> float:\n",
    "    \"\"\"\n",
    "    Map respondent choice to human_label:\n",
    "      - If model said Toxic (1):\n",
    "           'Agree'   -> 1\n",
    "           'Disagree'-> 0\n",
    "      - If model said Non-Toxic (0):\n",
    "           'Agree'   -> 0\n",
    "           'Disagree'-> 1\n",
    "      - 'Not Sure' or other -> np.nan\n",
    "    \"\"\"\n",
    "    r = str(resp).strip().lower()\n",
    "    if r.startswith('agree'):\n",
    "        return 1 if model_label_for_item == 1 else 0\n",
    "    if r.startswith('disagree'):\n",
    "        return 0 if model_label_for_item == 1 else 1\n",
    "    return np.nan  # Not Sure / blank\n",
    "\n",
    "# Join human responses with uncertain metadata by matching on the exact comment text\n",
    "if not hf_long.empty and not civil_uncertain.empty:\n",
    "    merged = hf_long.merge(\n",
    "        civil_uncertain,\n",
    "        on='comment_text',\n",
    "        how='left'\n",
    "    )\n",
    "    # derive human_label\n",
    "    merged['human_label'] = [\n",
    "        map_response_to_label(r, ml) if pd.notnull(ml) else np.nan\n",
    "        for r, ml in zip(merged['response'], merged['model_label'])\n",
    "    ]\n",
    "    # compute agreement score within comment (if multiple respondents per item)\n",
    "    agg = (merged\n",
    "           .groupby(['comment_text','model_prob','model_label'], as_index=False)\n",
    "           .agg(human_label=('human_label', 'mean')))  # mean over respondents (ignores NaNs)\n",
    "else:\n",
    "    agg = pd.DataFrame(columns=['comment_text','model_prob','model_label','human_label'])\n",
    "\n",
    "# Build corrected labels (Option 2 we discussed): backfill with model_label if human_label missing\n",
    "def build_corrected_df(agg_df):\n",
    "    df = agg_df.copy()\n",
    "    # round human_label to binary when present; keep NaN otherwise\n",
    "    df['human_binary'] = df['human_label'].apply(lambda v: 1 if pd.notnull(v) and v >= 0.5 else (0 if pd.notnull(v) else np.nan))\n",
    "    df['final_label']  = df['human_binary']\n",
    "    # backfill with model label when human missing\n",
    "    m = df['final_label'].isna()\n",
    "    df.loc[m, 'final_label'] = df.loc[m, 'model_label']\n",
    "    df['final_label'] = df['final_label'].astype(int)\n",
    "    return df[['comment_text','model_prob','model_label','human_label','final_label']].copy()\n",
    "\n",
    "corrected_df = build_corrected_df(agg)\n",
    "corrected_df.to_csv(\"corrected_human_feedback.csv\", index=False)\n",
    "print(f\"Human feedback rows combined: {len(corrected_df)} \"\n",
    "      f\"(with backfill to model labels where needed).\")\n",
    "\n",
    "# Ensure we have both classes; if not, you can inject a few non-toxic or toxic examples:\n",
    "def ensure_two_classes(df_corrected, civil_full_df, need_each_class=True, max_inject=10):\n",
    "    lbls = set(df_corrected['final_label'].unique().tolist())\n",
    "    if need_each_class and lbls == {0}:\n",
    "        # inject some toxic\n",
    "        pool = civil_full_df[civil_full_df['model_prob'] >= 0.80].copy()\n",
    "        pool = pool.sample(min(max_inject, len(pool)), random_state=RANDOM_STATE)\n",
    "        add = pool[['comment_text','model_prob','model_label']].copy()\n",
    "        add['human_label'] = np.nan\n",
    "        add['final_label'] = 1\n",
    "        return pd.concat([df_corrected, add], ignore_index=True)\n",
    "    if need_each_class and lbls == {1}:\n",
    "        # inject some non-toxic\n",
    "        pool = civil_full_df[civil_full_df['model_prob'] <= 0.20].copy()\n",
    "        pool = pool.sample(min(max_inject, len(pool)), random_state=RANDOM_STATE)\n",
    "        add = pool[['comment_text','model_prob','model_label']].copy()\n",
    "        add['human_label'] = np.nan\n",
    "        add['final_label'] = 0\n",
    "        return pd.concat([df_corrected, add], ignore_index=True)\n",
    "    return df_corrected\n",
    "\n",
    "corrected_df = ensure_two_classes(corrected_df, civil_df)\n",
    "print(\"Final label balance (corrected_df):\\n\", corrected_df['final_label'].value_counts(dropna=False))\n",
    "\n",
    "# =========================\n",
    "# 4) Retraining with Human-Corrected Items (Augment Training)\n",
    "# =========================\n",
    "if not corrected_df.empty:\n",
    "    X_hil  = vectorizer.transform(corrected_df['comment_text'])\n",
    "    y_hil  = corrected_df['final_label'].astype(int).reset_index(drop=True)\n",
    "    X_aug  = vstack([X_train_tfidf, X_hil])\n",
    "    y_aug  = pd.concat([y_train.reset_index(drop=True), y_hil], ignore_index=True)\n",
    "\n",
    "    retrained = LogisticRegression(max_iter=1000)\n",
    "    retrained.fit(X_aug, y_aug)\n",
    "\n",
    "    # Evaluate on the same toxic test set for apples-to-apples comparison\n",
    "    y_pred_hil = retrained.predict(X_test_tfidf)\n",
    "    y_prob_hil = retrained.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "    print(\"\\n=== After HIL Augmentation ===\")\n",
    "    print(classification_report(y_test, y_pred_hil, digits=3))\n",
    "    print(\"AUC:\", roc_auc_score(y_test, y_prob_hil))\n",
    "\n",
    "    # Comparison plot (Before vs After)\n",
    "    def compare_bars(y_true, y_pred_b, y_prob_b, y_pred_a, y_prob_a, path=\"fig_compare_before_after.png\"):\n",
    "        P_b = precision_score(y_true, y_pred_b)\n",
    "        R_b = recall_score(y_true, y_pred_b)\n",
    "        F_b = f1_score(y_true, y_pred_b)\n",
    "        A_b = roc_auc_score(y_true, y_prob_b)\n",
    "\n",
    "        P_a = precision_score(y_true, y_pred_a)\n",
    "        R_a = recall_score(y_true, y_pred_a)\n",
    "        F_a = f1_score(y_true, y_pred_a)\n",
    "        A_a = roc_auc_score(y_true, y_prob_a)\n",
    "\n",
    "        metrics = ['Precision','Recall','F1-score','AUC']\n",
    "        before  = [P_b, R_b, F_b, A_b]\n",
    "        after   = [P_a, R_a, F_a, A_a]\n",
    "\n",
    "        x = np.arange(len(metrics)); width = 0.35\n",
    "        plt.figure(figsize=(8,5))\n",
    "        b1 = plt.bar(x - width/2, before, width, label='Before')\n",
    "        b2 = plt.bar(x + width/2, after,  width, label='After')\n",
    "        plt.xticks(x, metrics); plt.ylim(0, 1.05)\n",
    "        for bars in (b1, b2):\n",
    "            for bar in bars:\n",
    "                v = bar.get_height()\n",
    "                plt.text(bar.get_x()+bar.get_width()/2, v+0.02, f\"{v:.2f}\", ha='center')\n",
    "        plt.ylabel('Score'); plt.title('Model Performance: Before vs After Human Feedback')\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout(); plt.savefig(path, dpi=300); plt.show()\n",
    "\n",
    "    compare_bars(y_test, y_pred_base, y_prob_base, y_pred_hil, y_prob_hil)\n",
    "\n",
    "else:\n",
    "    print(\"No human‑feedback rows parsed; skipping retrain step.\")\n",
    "\n",
    "# =========================\n",
    "# 5) Save Key Outputs\n",
    "# =========================\n",
    "pd.DataFrame({\n",
    "    'y_true': y_test.reset_index(drop=True),\n",
    "    'y_pred_baseline': y_pred_base,\n",
    "    'y_prob_baseline': y_prob_base\n",
    "}).to_csv(\"baseline_test_predictions.csv\", index=False)\n",
    "\n",
    "if 'y_pred_hil' in locals():\n",
    "    pd.DataFrame({\n",
    "        'y_true': y_test.reset_index(drop=True),\n",
    "        'y_pred_after': y_pred_hil,\n",
    "        'y_prob_after': y_prob_hil\n",
    "    }).to_csv(\"after_hil_test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb4757-2af0-4d9a-b7bc-54f6bb73d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Imports ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    ")\n",
    "import itertools\n",
    "\n",
    "# --- Small helper for pretty bars ---\n",
    "def _annotate_bars(ax):\n",
    "    for p in ax.patches:\n",
    "        h = p.get_height()\n",
    "        ax.annotate(f\"{h:.2f}\", (p.get_x()+p.get_width()/2, h),\n",
    "                    ha='center', va='bottom', xytext=(0,3), textcoords='offset points')\n",
    "\n",
    "# === 1) Plotters ===\n",
    "def plot_roc(y_true, y_score, title=\"ROC Curve\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    plt.figure(figsize=(5.2, 4.2))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0,1], [0,1], ls='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pr(y_true, y_score, title=\"Precision–Recall Curve\"):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "\n",
    "    plt.figure(figsize=(5.2,4.2))\n",
    "    plt.plot(recall, precision, lw=2, label=f\"AP = {ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title=\"Confusion Matrix (thr=0.5)\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    plt.figure(figsize=(5,4.2))\n",
    "    plt.imshow(cm, cmap='Greys')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    tick_labels = [\"Non-Toxic (0)\", \"Toxic (1)\"]\n",
    "    plt.xticks([0,1], tick_labels, rotation=15)\n",
    "    plt.yticks([0,1], tick_labels)\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\":        f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"auc\":       roc_auc_score(y_true, y_prob),\n",
    "        \"y_pred\":    y_pred\n",
    "    }\n",
    "\n",
    "def plot_comparison(baseline, hil, title=\"Model Performance: Before vs After HIL\"):\n",
    "    metrics = [\"precision\", \"recall\", \"f1\", \"auc\"]\n",
    "    before = [baseline[m] for m in metrics]\n",
    "    after  = [hil[m] for m in metrics]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    w = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(7,4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd391c9-94d6-49df-9ec0-ea6e363a6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ax.bar(x - w/2, before, w, label=\"Before HIL\")\n",
    "    ax.bar(x + w/2, after,  w, label=\"After HIL\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.capitalize() for m in metrics])\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    _annotate_bars(ax)\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === 2) BASELINE predictions & plots ===\n",
    "# y_test: ground truth (0/1)\n",
    "# model:     baseline LogisticRegression (already fit)\n",
    "# X_test_tfidf: TF-IDF of X_test (same features used to fit baseline model)\n",
    "y_prob_base = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "base = compute_metrics(y_test, y_prob_base, thr=0.5)\n",
    "\n",
    "print(\"=== Baseline (no HIL) ===\")\n",
    "print(classification_report(y_test, base[\"y_pred\"]))\n",
    "print(\"AUC:\", base[\"auc\"])\n",
    "\n",
    "plot_roc(y_test, y_prob_base, title=\"Baseline ROC (LogReg + TF‑IDF)\")\n",
    "plot_pr(y_test, y_prob_base, title=\"Baseline Precision–Recall\")\n",
    "plot_confusion(y_test, base[\"y_pred\"], title=\"Baseline Confusion Matrix (thr=0.5)\")\n",
    "\n",
    "# === 3) HIL (retrained) predictions & plots ===\n",
    "# retrained_model: LogisticRegression retrained with HIL-augmented data (already fit)\n",
    "y_prob_hil = retrained_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "hil = compute_metrics(y_test, y_prob_hil, thr=0.5)\n",
    "\n",
    "print(\"\\n=== After HIL (retrained) ===\")\n",
    "print(classification_report(y_test, hil[\"y_pred\"]))\n",
    "print(\"AUC:\", hil[\"auc\"])\n",
    "\n",
    "plot_roc(y_test, y_prob_hil, title=\"After HIL: ROC (Retrained)\")\n",
    "plot_pr(y_test, y_prob_hil, title=\"After HIL: Precision–Recall\")\n",
    "plot_confusion(y_test, hil[\"y_pred\"], title=\"After HIL: Confusion Matrix (thr=0.5)\")\n",
    "\n",
    "# === 4) Side-by-side comparison bar chart ===\n",
    "plot_comparison(base, hil, title=\"Before vs After Human Feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbba95-1ff6-4a27-8e0c-9e157350b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Inputs/assumptions ----------------------------------------------------\n",
    "identity_columns = [\n",
    "    'male','female','transgender','heterosexual',\n",
    "    'homosexual_gay_or_lesbian','bisexual','christian','jewish','muslim',\n",
    "    'black','white','asian','latino','psychiatric_or_mental_illness'\n",
    "]\n",
    "\n",
    "# Ensure required cols exist\n",
    "missing = [c for c in identity_columns + ['comment_text','target'] if c not in civil_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in civil_df: {missing}\")\n",
    "\n",
    "# Binarize ground truth for fairness eval (Civil Comments 'target' is 0..1)\n",
    "y_true_civil = (civil_df['target'] >= 0.5).astype(int).values\n",
    "\n",
    "# Re-use the SAME TF-IDF vectorizer\n",
    "X_civil_tfidf = vectorizer.transform(civil_df['comment_text'])\n",
    "\n",
    "# Get probabilities from baseline and HIL models\n",
    "y_prob_civil_base = model.predict_proba(X_civil_tfidf)[:, 1]\n",
    "y_prob_civil_hil  = retrained_model.predict_proba(X_civil_tfidf)[:, 1]\n",
    "\n",
    "# --- 2) Helper metrics (Borkan et al., 2019) ----------------------------------\n",
    "def safe_auc(y_true, y_score):\n",
    "    # handle edge cases where only one class is present\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return np.nan\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "def subgroup_auc(y_true, y_score, subgroup_mask):\n",
    "    return safe_auc(y_true[subgroup_mask], y_score[subgroup_mask])\n",
    "\n",
    "def bpsn_auc(y_true, y_score, subgroup_mask):\n",
    "    # Background Positive, Subgroup Negative\n",
    "    # positives from background (mask==False & y_true==1) vs negatives from subgroup (mask==True & y_true==0)\n",
    "    idx = ((~subgroup_mask) & (y_true == 1)) | ((subgroup_mask) & (y_true == 0))\n",
    "    return safe_auc(y_true[idx], y_score[idx])\n",
    "\n",
    "def bnsp_auc(y_true, y_score, subgroup_mask):\n",
    "    # Background Negative, Subgroup Positive\n",
    "    # negatives from background vs positives from subgroup\n",
    "    idx = ((~subgroup_mask) & (y_true == 0)) | ((subgroup_mask) & (y_true == 1))\n",
    "    return safe_auc(y_true[idx], y_score[idx])\n",
    "\n",
    "def average_equality_gap(y_true, y_score, subgroup_mask):\n",
    "    # AEG ≈ mean difference of scores for positives (or negatives) between subgroup and background.\n",
    "    # Here we use positives-based AEG (you can also compute for negatives and average both).\n",
    "    pos = y_true == 1\n",
    "    s_pos = y_score[subgroup_mask & pos]\n",
    "    b_pos = y_score[(~subgroup_mask) & pos]\n",
    "    if len(s_pos)==0 or len(b_pos)==0:\n",
    "        return np.nan\n",
    "    return float(np.mean(s_pos) - np.mean(b_pos))\n",
    "\n",
    "# --- 3) Compute metrics for each subgroup for both models ---------------------\n",
    "def fairness_table(y_true, y_score, df, id_cols):\n",
    "    rows = []\n",
    "    for col in id_cols:\n",
    "        mask = df[col].astype(float) >= 0.5  # Civil Comments marks presence as float ∈ [0,1]\n",
    "        rows.append({\n",
    "            \"subgroup\": col,\n",
    "            \"count\": int(mask.sum()),\n",
    "            \"subgroup_auc\": subgroup_auc(y_true, y_score, mask),\n",
    "            \"bpsn_auc\":     bpsn_auc(y_true, y_score, mask),\n",
    "            \"bnsp_auc\":     bnsp_auc(y_true, y_score, mask),\n",
    "            \"aeg_pos\":      average_equality_gap(y_true, y_score, mask),\n",
    "            \"avg_prob_in_subgroup\": float(np.mean(y_score[mask])) if mask.sum()>0 else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"subgroup\")\n",
    "\n",
    "fair_base = fairness_table(y_true_civil, y_prob_civil_base, civil_df, identity_columns)\n",
    "fair_hil  = fairness_table(y_true_civil, y_prob_civil_hil,  civil_df, identity_columns)\n",
    "\n",
    "# Merge for comparison\n",
    "fair_compare = fair_base.merge(fair_hil, on=\"subgroup\", suffixes=(\"_base\",\"_hil\"))\n",
    "# Compute deltas (HIL - Base)\n",
    "for m in [\"subgroup_auc\",\"bpsn_auc\",\"bnsp_auc\",\"aeg_pos\",\"avg_prob_in_subgroup\"]:\n",
    "    fair_compare[f\"delta_{m}\"] = fair_compare[f\"{m}_hil\"] - fair_compare[f\"{m}_base\"]\n",
    "\n",
    "# Display the top-line comparison\n",
    "cols_to_show = [\n",
    "    \"subgroup\",\"count_base\",\"subgroup_auc_base\",\"bpsn_auc_base\",\"bnsp_auc_base\",\"aeg_pos_base\",\n",
    "    \"subgroup_auc_hil\",\"bpsn_auc_hil\",\"bnsp_auc_hil\",\"aeg_pos_hil\",\n",
    "    \"delta_subgroup_auc\",\"delta_bpsn_auc\",\"delta_bnsp_auc\",\"delta_aeg_pos\"\n",
    "]\n",
    "# count is the same for both (same df), rename for clarity\n",
    "fair_compare[\"count_base\"] = fair_compare[\"count_base\"] if \"count_base\" in fair_compare else fair_compare[\"count\"]\n",
    "\n",
    "print(\"\\n=== Fairness metrics per subgroup (Baseline vs HIL) ===\")\n",
    "display(fair_compare[cols_to_show])\n",
    "\n",
    "# --- 4) Quick visualization: change in BPSN/BNSP/Subgroup AUC -----------------\n",
    "metrics_for_bar = [\"delta_subgroup_auc\",\"delta_bpsn_auc\",\"delta_bnsp_auc\"]\n",
    "plot_df = fair_compare[[\"subgroup\"] + metrics_for_bar].set_index(\"subgroup\").sort_values(\"delta_subgroup_auc\")\n",
    "\n",
    "ax = plot_df.plot(kind=\"barh\", figsize=(10,7))\n",
    "ax.set_title(\"Change in Fairness Metrics After HIL (HIL – Baseline)\")\n",
    "ax.set_xlabel(\"Delta AUC (positive = improvement)\")\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e5286-f299-4a6b-9dbf-79d473dc0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== 1) Plug your final numbers here =====\n",
    "# Example placeholders — replace with your measured values\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Subgroup AUC\", \"BPSN AUC\", \"BNSP AUC\"]\n",
    "\n",
    "baseline = [0.95, 0.92, 0.64, 0.76, 0.78, 0.77, 0.79]  # ← replace\n",
    "hil      = [0.95, 0.92, 0.66, 0.77, 0.79, 0.77, 0.81]  # ← replace\n",
    "\n",
    "# ===== 2) Radar chart prep =====\n",
    "angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
    "baseline_vals = np.r_[baseline, baseline[0]]\n",
    "hil_vals      = np.r_[hil, hil[0]]\n",
    "angles_full   = np.r_[angles, angles[0]]\n",
    "\n",
    "# ===== 3) Plot =====\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "ax.plot(angles_full, baseline_vals, linewidth=2, label=\"Baseline\")\n",
    "ax.fill(angles_full, baseline_vals, alpha=0.15)\n",
    "\n",
    "ax.plot(angles_full, hil_vals, linewidth=2, linestyle=\"--\", label=\"HIL\")\n",
    "ax.fill(angles_full, hil_vals, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_yticks([0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "ax.set_yticklabels([\"0.60\",\"0.70\",\"0.80\",\"0.90\",\"1.00\"])\n",
    "ax.set_ylim(0.6, 1.0)\n",
    "ax.set_title(\"Baseline vs HIL – Final Metrics (Radar View)\", pad=20)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25, 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
